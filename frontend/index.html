<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <link rel="icon" type="image/svg+xml" href="/src/favicon.ico" />
    <meta
      name="viewport"
      content="minimum-scale=1, initial-scale=1, width=device-width, user-scalable=no"
    />
    <meta name="description" content="This is a set of fine grained test cases(prompts) to benchmark the reasoning and generation of LLMs in complex scenarios. Please note that this benchmark does NOT mean to be a comprehensive test against LLMs. This project is originated from an internal project of babel.cloud, the purpose of the project is to test LLMs' performance in context understanding and instruction compliance. ">
    <meta name="keywords" content="LLM, Benchmark, GPT, OpenAI, GPT-3.5, GPT-4">
    <meta name="author" content="https://babel.cloud">
    <title>LLM-RGB</title>
  </head>
  <body style="background-color: #12171D;">
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
